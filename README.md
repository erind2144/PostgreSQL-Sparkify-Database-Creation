### Date Created
This project was created on 1/21/2022

### Project Title
Postgres Database Schema and ETL Pipeline Creation for Sparkify

### Description
I used Python to create a database schema and ETL pipeline for a fictional
startup company called Sparkify. I defined fact and dimension tables for a star
schema and wrote an ETL pipeline that transfers data from files in two local
directories into these tables in Postgres using Python and SQL.

### Instructions
Run the python files in the terminal in the following order:
1. sql_queries.py
2. create_tables.py
3. etl.py

### File Descriptions
**sql_queries.py** - Contains all of the queries to drop tables, create tables,
and insert rows into those tables. It also contains a song_select query used to
find the song_id and artist_id for songs to match to the rows in the dataframe
and subsequently insert the relevant data in the songplays table. Lastly, it creates a list containing all of the create table queries as well as a list of all the drop table queries.

**create_tables.py** - Defines the following functions: *create_database()* which creates and connects to the sparkifydb and also returns the connection and cursor to sparkifydb,
*drop_tables(cur, conn)* which drops each table using the queries in the 'drop_table_queries' list, *create_tables(cur, conn)* which creates each table using the queries in the 'create_table_queries' list, and *main()* which drops (if necessary) and creates the sparkify database, establishes connection with the sparkify database and gets cursor to it, drops all the tables, creates all tables needed, and closes the connection. If _name_ == "_main_",
it will run main() which utilizes all of the previously defined functions.

**etl.py** - Defines the following functions: *process_song_file(cur, datafile)* which opens the song file, inserts the song record, and inserts the artist record, *process_log_file(cur, datafile)* which opens and filters the log file, inserts time data records, inserts user records, and inserts songplay records, *process_data(cur, conn, filepath, func)* which gets all files matching extension from directory, get total number of files found, and iterates over files and process, and *main()* which creates the connection and cursor, processes all the data files, and closes the connection. If _name_ == "_main_", it will run main().

**sparkifydb_erd.png** - Entity Relationship Diagram for the newly created Sparkify Database.

**song_data** - A subset of real data from the Million Song Dataset (http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.  

**log_data** - Dataset that consists of log files in JSON format generated by an event simulator (https://github.com/Interana/eventsim) based on the songs in the song_data dataset. These simulate activity logs from a music streaming app based on specific configurations. The log files in this dataset are partitioned by year and month.

### Credits
This project was created for the Udacity Data Engineering Nanodegree. I referenced content presented in this course for this project.  

Additionally, I referenced the PostgreSQL Documentation.

Data from the Million Song Dataset was used in addition to data generated by an event simulator developed by Interana, Inc.
